---
title: "Simulations in Data Science- Applications and Implications"
---

Ever since my Datascience 1 course covered the topic of simulations in data I’ve been thinking about the implications and applications of this tool.  We were taught to use simulations as a means of control, or verification of the validity of a dataset. The way we were taught to do so was through using a code block such as:
```{r, warning=FALSE, message=FALSE}
library(mdsr)
library(tidyr)
library(ggplot2)
```

```{r}
uniform <- data.frame(u = runif(1000))
ggplot(uniform, aes(x = u)) +
geom_histogram()
```

Which prints a histogram of a random dataframe of 1000 values. Taking a step deeper, we looked at restaurants and sanitation grades across New York City using the Violations package in R. Dr. Sweeney’s theory was that restaurants close to the cutoff between A and B, for example, would be more likely to receive A’s as a result of some underlying (and potentially malicious) confound. 

While you would expect there to be a normal distribution of scores across the grades for restaurant violations, there seems to be a cluster slightly above the benchmarks for grades (i.e. there are significantly more restaurants with the lowest A score, than the highest B score). We looked at what a normal distribution of scores would look like using simulations, and then compared the actual values to the “expected” or simulated in the following code block:
```{r}
#setup to isolate the scores
minval <- 7
maxval <- 19
JustScores <- Violations %>%
filter(score >= minval & score <= maxval)%>%
select(dba, score) %>%
unique()
#creating the simulation
scores <- tally(~score, data = JustScores)
RandomFlip <- do(1000) * rflip(scores["13"] + scores["14"])
#plotting the simulated data vs actual
g <- ggplot(data = RandomFlip, aes(x = heads)) +
geom_histogram(binwidth = 5) +
xlim(c(2100, NA)) +
geom_vline(xintercept = scores["14"], col = 'red', size = 2) +
annotate("text", x = 2137, y = 45, label = 'observed', hjust = 'left') +
xlab("Number of restaurants with scores of 14 (if equal probability)")
g
```

We found that there is a huge discrepancy in the number of A’s and B’s in the simulated dataset, when compared with the real one. This application of simulations seems promising, but to me the most interesting application of simulations is in understanding trends, specifically ones that humans have trouble tracking, in data.

From my limited experience a lot of the job of a biostatistician is to walk the tightrope between data that is poor, and cannot be salvaged, and data that can be framed in a way that it becomes statistically significant. Simulations in my mind narrow this gap. If you are able to compensate for missing values, for example, through replicating the “paths” a dataset without missing values would take, and then to make an educated guess on what the actual dataset would look like, you can turn a flawed, sloppy, hole-filled research project into a statistically significant and seemingly valid study. 

I don’t have a strong understanding of where the lines are in this type of research, but I hope that ethical (and legal) considerations prevent this from happening in excess. In my work as a research assistant with the INSIGHT Network I've seen first hand how highly researchers prioritize significant results. I hope to learn more, both about uses and optimizations for simulations in data, and in the infrastructure in place to hold poor data accountable.

Scientific research is a field that I'm passionate about, but throughout my experiences in research I've heard and seen many stories regarding steps taken to avoid the repercussions of insignificant findings. I've had researchers tell me that another researcher's findings couldn't be used or studied because they were prone to fabricating results. I've been asked to work on interim reports for clinical trials that hadn't started. I attribute the lack of accountability in scientific research, in part due to the complexity and difficulty in reproducing mechanistic studies, and in part due to poor processes in peer review and analysis, to one of the main reasons why we know so little about the etiology of so many diseases. We've known about Alzheimer's disease since the 1970's and are nowhere near understanding it's root cause. Mental disorders, cancers, and many, many more widespread and debilitating diseases that have wrought havoc on our society are poorly understood because of these systemic issues. 
The question becomes one of responsibility in multiple different lights: Who is or should be responsible for validating results? Who is responsible for analysis? Who is responsible for unmasking attempts at hiding poor practices? Who is responsible for repeating research? Who is responsible for paying for studies to confirm  results? Who is responsible for dissemination? Who is responsible for linking similar studies and prompting subsequent work into important areas? 

These are questions that to me don't currently have answers in the field of scientific research. There are pockets or echo chambers where systems are in place that answer some or many of them, but there is no standardized enforcement agency in Alzheimer's research. There is no "study replicating team" in cancer research. This is a crucially unmet need in the realm of scientific research, and one that I think biostatisticians can use statistical literacy to play an active role in filling, but who is responsible for telling them to do so? 